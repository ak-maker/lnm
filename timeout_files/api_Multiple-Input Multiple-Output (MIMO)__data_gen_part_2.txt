# Multiple-Input Multiple-Output (MIMO)

## Detection
### class: sionna.mimo.KBestDetector(output, num_streams, k, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_real_rep=False, list2llr=None, dtype=tf.complex64)
### class: sionna.mimo.LinearDetector(equalizer, output, demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)
### class: sionna.mimo.MaximumLikelihoodDetector(output, demapping_method, num_streams, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, with_prior=False, dtype=tf.complex64, **kwargs)

# Multiple-Input Multiple-Output (MIMO)

This module provides layers and functions to support simulation of multicell MIMO transmissions.

### class: sionna.mimo.KBestDetector(output, num_streams, k, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, use_real_rep=False, list2llr=None, dtype=tf.complex64)
MIMO K-Best detector
This layer implements K-Best MIMO detection as described in (Eq. 4-5) [FT2015]. It can either generate hard decisions (for symbols or bits) or compute LLRs.
The algorithm operates in either the complex or real-valued domain. Although both options produce identical results, the former has the advantage that it can be applied to arbitrary non-QAM constellations. It also reduces the number of streams (or depth) by a factor of two.
The way soft-outputs (i.e., LLRs) are computed is determined by the list2llr function. The default solution List2LLRSimple assigns a predetermined value to all LLRs without counter-hypothesis.
This layer assumes the following channel model:
\mathbf{y} = \mathbf{H}\mathbf{x} + \mathbf{n}
where
\mathbf{y}\in\mathbb{C}^M
is the received signal vector,
\mathbf{x}\in\mathcal{C}^S
is the vector of transmitted symbols which are uniformly and independently drawn from the constellation
\mathcal{C}
,
\mathbf{H}\in\mathbb{C}^{M\times S}
is the known channel matrix, and
\mathbf{n}\in\mathbb{C}^M
is a complex Gaussian noise vector. It is assumed that
\mathbb{E}\left[\mathbf{n}\right]=\mathbf{0}
and
\mathbb{E}\left[\mathbf{n}\mathbf{n}^{\mathsf{H}}\right]=\mathbf{S}
, where
\mathbf{S}
has full rank.
In a first optional step, the channel model is converted to its real-valued equivalent, see complex2real_channel(). We assume in the sequel the complex-valued representation. Then, the channel is whitened using whiten_channel():
\begin{split}\tilde{\mathbf{y}} &= \mathbf{S}^{-\frac{1}{2}}\mathbf{y}\\
&=  \mathbf{S}^{-\frac{1}{2}}\mathbf{H}\mathbf{x} + \mathbf{S}^{-\frac{1}{2}}\mathbf{n}\\
&= \tilde{\mathbf{H}}\mathbf{x} + \tilde{\mathbf{n}}.\end{split}
.
Next, the columns of
\tilde{\mathbf{H}}
are sorted according to their norm in descending order. Then, the QR decomposition of the resulting channel matrix is computed:
\tilde{\mathbf{H}} = \mathbf{Q}\mathbf{R}
where
\mathbf{Q}\in\mathbb{C}^{M\times S}
is unitary and
\mathbf{R}\in\mathbb{C}^{S\times S}
is upper-triangular. The channel outputs are then pre-multiplied by
\mathbf{Q}^{\mathsf{H}}
. This leads to the final channel model on which the K-Best detection algorithm operates:
\bar{\mathbf{y}} = \mathbf{R}\bar{\mathbf{x}} + \bar{\mathbf{n}}
where
\bar{\mathbf{y}}\in\mathbb{C}^S
,
\bar{\mathbf{x}}\in\mathbb{C}^S
, and
\bar{\mathbf{n}}\in\mathbb{C}^S
with
\mathbb{E}\left[\bar{\mathbf{n}}\right]=\mathbf{0}
and
\mathbb{E}\left[\bar{\mathbf{n}}\bar{\mathbf{n}}^{\mathsf{H}}\right]=\mathbf{I}
.
LLR Computation
The K-Best algorithm produces
K
candidate solutions
\bar{\mathbf{x}}_k\in\mathcal{C}^S
and their associated distance metrics
d_k=\lVert \bar{\mathbf{y}} - \mathbf{R}\bar{\mathbf{x}}_k \rVert^2
for
k=1,\dots,K
. If the real-valued channel representation is used, the distance metrics are scaled by 0.5 to account for the reduced noise power in each complex dimension. A hard-decision is simply the candidate with the shortest distance. Various ways to compute LLRs from this list (and possibly additional side-information) are possible. The (sub-optimal) default solution is List2LLRSimple. Custom solutions can be provided.
Parameters
output (One of ["bit", "symbol"], str) – The type of output, either bits or symbols. Whether soft- or hard-decisions are returned can be configured with the hard_out flag.
num_streams (tf.int) – Number of transmitted streams
k (tf.int) – The number of paths to keep. Cannot be larger than the number of constellation points to the power of the number of streams.
constellation_type (One of ["qam", "pam", "custom"], str) – For “custom”, an instance of Constellation must be provided.
num_bits_per_symbol (int) – The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for constellation_type in [“qam”, “pam”].
constellation (Constellation) – An instance of Constellation or None. In the latter case, constellation_type and num_bits_per_symbol must be provided.
hard_out (bool) – If True, the detector computes hard-decided bit values or constellation point indices instead of soft-values. Defaults to False. The detector cannot compute soft-symbols.
use_real_rep (bool) – If True, the detector use the real-valued equivalent representation of the channel. Note that this only works with a QAM constellation. Defaults to False.
list2llr (None or instance of List2LLR) – The function to be used to compute LLRs from a list of candidate solutions. If None, the default solution List2LLRSimple is used.
dtype (One of [tf.complex64, tf.complex128] tf.DType (dtype)) – The dtype of y. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).
Input
(y, h, s) – Tuple:
y ([…,M], tf.complex) – 1+D tensor containing the received signals
h ([…,M,num_streams], tf.complex) – 2+D tensor containing the channel matrices
s ([…,M,M], tf.complex) – 2+D tensor containing the noise covariance matrices
Output
One of
[…,num_streams,num_bits_per_symbol], tf.float – LLRs or hard-decisions for every bit of every stream, if output equals “bit”
[…,num_streams,2**num_points], tf.float or […,num_streams], tf.int – Logits or hard-decisions for constellation symbols for every stream, if output equals “symbol” Hard-decisions correspond to the symbol indices.
Note
If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sionna.Config.xla_compat=true. See xla_compat.
### class: sionna.mimo.LinearDetector(equalizer, output, demapping_method, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, dtype=tf.complex64, **kwargs)
Convenience class that combines an equalizer, such as lmmse_equalizer(), and a Demapper.
Parameters
equalizer (str, one of ["lmmse", "zf", "mf"], or an equalizer function) – The equalizer to be used. Either one of the existing equalizers lmmse_equalizer(), zf_equalizer(), or mf_equalizer() can be used, or a custom equalizer callable provided that has the same input/output specification.
output (One of ["bit", "symbol"], str) – The type of output, either LLRs on bits or logits on constellation symbols.
demapping_method (One of ["app", "maxlog"], str) – The demapping method used.
constellation_type (One of ["qam", "pam", "custom"], str) – For “custom”, an instance of Constellation must be provided.
num_bits_per_symbol (int) – The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for constellation_type in [“qam”, “pam”].
constellation (Constellation) – An instance of Constellation or None. In the latter case, constellation_type and num_bits_per_symbol must be provided.
hard_out (bool) – If True, the detector computes hard-decided bit values or constellation point indices instead of soft-values. Defaults to False.
dtype (One of [tf.complex64, tf.complex128] tf.DType (dtype)) – The dtype of y. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).
Input
(y, h, s) – Tuple:
y ([…,M], tf.complex) – 1+D tensor containing the received signals
h ([…,M,num_streams], tf.complex) – 2+D tensor containing the channel matrices
s ([…,M,M], tf.complex) – 2+D tensor containing the noise covariance matrices
Output
One of
[…, num_streams, num_bits_per_symbol], tf.float – LLRs or hard-decisions for every bit of every stream, if output equals “bit”
[…, num_streams, num_points], tf.float or […, num_streams], tf.int – Logits or hard-decisions for constellation symbols for every stream, if output equals “symbol” Hard-decisions correspond to the symbol indices.
Note
If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you might need to set sionna.Config.xla_compat=true. This depends on the chosen equalizer function. See xla_compat.
### class: sionna.mimo.MaximumLikelihoodDetector(output, demapping_method, num_streams, constellation_type=None, num_bits_per_symbol=None, constellation=None, hard_out=False, with_prior=False, dtype=tf.complex64, **kwargs)
MIMO maximum-likelihood (ML) detector. If the with_prior flag is set, prior knowledge on the bits or constellation points is assumed to be available.
This layer implements MIMO maximum-likelihood (ML) detection assuming the following channel model:
\mathbf{y} = \mathbf{H}\mathbf{x} + \mathbf{n}
where
\mathbf{y}\in\mathbb{C}^M
is the received signal vector,
\mathbf{x}\in\mathcal{C}^K
is the vector of transmitted symbols which are uniformly and independently drawn from the constellation
\mathcal{C}
,
\mathbf{H}\in\mathbb{C}^{M\times K}
is the known channel matrix, and
\mathbf{n}\in\mathbb{C}^M
is a complex Gaussian noise vector. It is assumed that
\mathbb{E}\left[\mathbf{n}\right]=\mathbf{0}
and
\mathbb{E}\left[\mathbf{n}\mathbf{n}^{\mathsf{H}}\right]=\mathbf{S}
, where
\mathbf{S}
has full rank. If the with_prior flag is set, it is assumed that prior information of the transmitted signal
\mathbf{x}
is available, provided either as LLRs on the bits mapped onto
\mathbf{x}
or as logits on the individual constellation points forming
\mathbf{x}
.
Prior to demapping, the received signal is whitened:
\begin{split}\tilde{\mathbf{y}} &= \mathbf{S}^{-\frac{1}{2}}\mathbf{y}\\
&=  \mathbf{S}^{-\frac{1}{2}}\mathbf{H}\mathbf{x} + \mathbf{S}^{-\frac{1}{2}}\mathbf{n}\\
&= \tilde{\mathbf{H}}\mathbf{x} + \tilde{\mathbf{n}}\end{split}
The layer can compute ML detection of symbols or bits with either soft- or hard-decisions. Note that decisions are computed symbol-/bit-wise and not jointly for the entire vector
\textbf{x}
(or the underlying vector of bits).
ML detection of bits:
Soft-decisions on bits are called log-likelihood ratios (LLR). With the “app” demapping method, the LLR for the
i\text{th}
bit of the
k\text{th}
user is then computed according to
\begin{split}\begin{align}
    LLR(k,i)&= \ln\left(\frac{\Pr\left(b_{k,i}=1\lvert \mathbf{y},\mathbf{H}\right)}{\Pr\left(b_{k,i}=0\lvert \mathbf{y},\mathbf{H}\right)}\right)\\
            &=\ln\left(\frac{
            \sum_{\mathbf{x}\in\mathcal{C}_{k,i,1}} \exp\left(
                -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2
                \right) \Pr\left( \mathbf{x} \right)
            }{
            \sum_{\mathbf{x}\in\mathcal{C}_{k,i,0}} \exp\left(
                -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2
                \right) \Pr\left( \mathbf{x} \right)
            }\right)
\end{align}\end{split}
where
\mathcal{C}_{k,i,1}
and
\mathcal{C}_{k,i,0}
are the sets of vectors of constellation points for which the
i\text{th}
bit of the
k\text{th}
user is equal to 1 and 0, respectively.
\Pr\left( \mathbf{x} \right)
is the prior distribution of the vector of constellation points
x
. Assuming that the constellation points and bit levels are independent, it is computed from the prior of the bits according to
\Pr\left( \mathbf{x} \right) = \prod_{k=1}^K \prod_{i=1}^{I} \sigma \left( LLR_p(k,i) \right)
where
LLR_p(k,i)
is the prior knowledge of the
i\text{th}
bit of the
k\text{th}
user given as an LLR and which is set to
0
if no prior knowledge is assumed to be available, and
\sigma\left(\cdot\right)
is the sigmoid function. The definition of the LLR has been chosen such that it is equivalent with that of logit. This is different from many textbooks in communications, where the LLR is defined as
LLR(k,i) = \ln\left(\frac{\Pr\left(b_{k,i}=0\lvert \mathbf{y},\mathbf{H}\right)}{\Pr\left(b_{k,i}=1\lvert \mathbf{y},\mathbf{H}\right)}\right)
.
With the “maxlog” demapping method, the LLR for the
i\text{th}
bit of the
k\text{th}
user is approximated like
\begin{split}\begin{align}
    LLR(k,i) \approx&\ln\left(\frac{
        \max_{\mathbf{x}\in\mathcal{C}_{k,i,1}} \left( \exp\left(
            -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2
            \right) \Pr\left( \mathbf{x} \right) \right)
        }{
        \max_{\mathbf{x}\in\mathcal{C}_{k,i,0}} \left( \exp\left(
            -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2
            \right) \Pr\left( \mathbf{x} \right) \right)
        }\right)\\
        = &\min_{\mathbf{x}\in\mathcal{C}_{k,i,0}} \left( \left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2 - \ln \left(\Pr\left( \mathbf{x} \right) \right) \right) -
            \min_{\mathbf{x}\in\mathcal{C}_{k,i,1}} \left( \left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2 - \ln \left( \Pr\left( \mathbf{x} \right) \right) \right).
    \end{align}\end{split}
ML detection of symbols:
Soft-decisions on symbols are called logits (i.e., unnormalized log-probability).
With the “app” demapping method, the logit for the constellation point
c \in \mathcal{C}
of the
k\text{th}
user is computed according to
\begin{align}
    \text{logit}(k,c) &= \ln\left(\sum_{\mathbf{x} : x_k = c} \exp\left(
                -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2
                \right)\Pr\left( \mathbf{x} \right)\right).
\end{align}
With the “maxlog” demapping method, the logit for the constellation point
c \in \mathcal{C}
of the
k\text{th}
user is approximated like
\text{logit}(k,c) \approx \max_{\mathbf{x} : x_k = c} \left(
        -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2 + \ln \left( \Pr\left( \mathbf{x} \right) \right)
        \right).
When hard decisions are requested, this layer returns for the
k
th stream
\hat{c}_k = \underset{c \in \mathcal{C}}{\text{argmax}} \left( \sum_{\mathbf{x} : x_k = c} \exp\left(
                -\left\lVert\tilde{\mathbf{y}}-\tilde{\mathbf{H}}\mathbf{x}\right\rVert^2
                \right)\Pr\left( \mathbf{x} \right) \right)
where
\mathcal{C}
is the set of constellation points.
Parameters
output (One of ["bit", "symbol"], str) – The type of output, either LLRs on bits or logits on constellation symbols.
demapping_method (One of ["app", "maxlog"], str) – The demapping method used.
num_streams (tf.int) – Number of transmitted streams
constellation_type (One of ["qam", "pam", "custom"], str) – For “custom”, an instance of Constellation must be provided.
num_bits_per_symbol (int) – The number of bits per constellation symbol, e.g., 4 for QAM16. Only required for constellation_type in [“qam”, “pam”].
constellation (Constellation) – An instance of Constellation or None. In the latter case, constellation_type and num_bits_per_symbol must be provided.
hard_out (bool) – If True, the detector computes hard-decided bit values or constellation point indices instead of soft-values. Defaults to False.
with_prior (bool) – If True, it is assumed that prior knowledge on the bits or constellation points is available. This prior information is given as LLRs (for bits) or log-probabilities (for constellation points) as an additional input to the layer. Defaults to False.
dtype (One of [tf.complex64, tf.complex128] tf.DType (dtype)) – The dtype of y. Defaults to tf.complex64. The output dtype is the corresponding real dtype (tf.float32 or tf.float64).
Input
(y, h, s) or (y, h, prior, s) – Tuple:
y ([…,M], tf.complex) – 1+D tensor containing the received signals.
h ([…,M,num_streams], tf.complex) – 2+D tensor containing the channel matrices.
prior ([…,num_streams,num_bits_per_symbol] or […,num_streams,num_points], tf.float) – Prior of the transmitted signals. If output equals “bit”, then LLRs of the transmitted bits are expected. If output equals “symbol”, then logits of the transmitted constellation points are expected. Only required if the with_prior flag is set.
s ([…,M,M], tf.complex) – 2+D tensor containing the noise covariance matrices.
Output
One of
[…, num_streams, num_bits_per_symbol], tf.float – LLRs or hard-decisions for every bit of every stream, if output equals “bit”.
[…, num_streams, num_points], tf.float or […, num_streams], tf.int – Logits or hard-decisions for constellation symbols for every stream, if output equals “symbol”. Hard-decisions correspond to the symbol indices.
Note
If you want to use this layer in Graph mode with XLA, i.e., within a function that is decorated with @tf.function(jit_compile=True), you must set sionna.Config.xla_compat=true. See xla_compat.
